[["index.html", "1 Welcome to S2Y Lab 3 1.1 Introduction", " S2Y Lab 3 Linear models in vector-matrix form 1 Welcome to S2Y Lab 3 Intended Learning Outcomes: write down linear models in vector-matrix form; obtain the design matrix using R; obtain least squares estimates of model parameters using vector-matrix form in R; obtain the residual sum of squares using vector-matrix form in R. 1.1 Introduction In the lectures we learned how to write down linear models in a general way using vector-matrix notation: \\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon \\quad\\quad\\quad \\text{or} \\quad\\quad\\quad \\mathbb{E}(\\mathbf{Y}) = \\mathbf{X}\\boldsymbol\\beta\\] where \\(\\mathbf{Y}\\) is the \\((n \\times 1)\\) vector of observations; \\(\\boldsymbol{\\beta}\\) is the \\((p \\times 1)\\) vector of parameters; \\(\\mathbf{X}\\) is an \\((n \\times p)\\) , with known constants as elements; and \\(\\boldsymbol{\\epsilon}\\) is the \\((n \\times 1)\\) vector of random errors. TASK 1 Without looking at the lecture notes, write down the simple linear regression model, i.e. \\[\\begin{equation} Y_i = \\alpha + \\beta x_i + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2), \\quad i=1,\\ldots,n \\tag{1.1} \\end{equation}\\] using vector-matrix notation: \\[\\underbrace{\\begin{bmatrix} &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ \\end{bmatrix}}_{\\mathbf{Y}} = \\underbrace{\\begin{bmatrix} &amp; &amp; &amp; &amp; \\\\ &amp; &amp; &amp; &amp; \\\\ &amp; &amp; &amp; &amp; \\\\ &amp; &amp; &amp; &amp; \\\\ &amp; &amp; &amp; &amp; \\\\ \\end{bmatrix}}_{\\mathbf{X}} \\underbrace{\\begin{bmatrix} &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ \\end{bmatrix}}_{\\boldsymbol\\beta} + \\underbrace{\\begin{bmatrix} &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ \\end{bmatrix}}_{\\boldsymbol\\epsilon} \\] In lectures we also looked at general formulas for obtaining the least squares estimates of the model parameters in terms of vector-matrix notation, which is given as \\[\\begin{equation} \\boldsymbol{\\hat{\\beta}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{Y} \\tag{1.2} \\end{equation}\\] and estimate of the error variance, which is given as \\[\\begin{equation} \\hat{\\sigma}^2=\\frac{\\text{RSS}}{n-p} =\\frac{\\mathbf{Y}^\\top\\mathbf{Y}-\\mathbf{Y}^\\top\\mathbf{X}\\boldsymbol{\\hat{\\beta}}}{n-p} \\tag{1.3} \\end{equation}\\] "],["example-1-multiple-linear-regression.html", "2 Example 1: Multiple linear regression 2.1 Least squares estimates of model parameters using vector-matrix form", " 2 Example 1: Multiple linear regression We briefly looked at this data in the lectures. Here 50 US states were investigated in terms of their crime rates (per 100,000 people), which includes crimes such as murder, assault, and car theft. Some demographic information about each state was also recorded, such as the number of police and prisoners per 100,000 people, the percentage of population living in poverty, and the percentage of high school dropouts (i.e. 16-19 year olds who were not in school and did not finish the 12\\(^\\text{th}\\) grade). The question of interest is whether we can predict US crime rates from the high school dropout rates and other predictors? The data are available from the csv file crime.csv and contain six columns, described as follows: C1 C1.T US state C2 Crime Crime rate per 100,000 C3 Police Number of police per 100,000 C4 Prison Number of prisoners per 100,000 C5 Poverty Percentage of population living in poverty C6 Dropout Percentage of high school dropouts QUESTION 1: Use plot or pairs to visualise the data and determine which predictors may be useful in predicting Crime. Solution The R command pairs() may be used to see the relationships between all variables. crime &lt;- read.csv(&quot;crime.csv&quot;) pairs(crime[,-1], lower.panel = NULL) # We add [,-1] to the end of crime to remove the first column which has non-numeric arguments (state names) Apart from Dropout which has been discussed in the lectures, there may also be a positive linear relationship between Crime and Police and between Crime and Prison, though the relationship doesn't seem to be very strong. Build a simple linear regression model with Dropout as the predictor and interpret estimated coefficients. According to the model, when Dropout is equal to 0, the crime rate would be roughly . For every 1% increase in the % of high school dropouts, the expected crime rate (per 100,000) would increasedecrease by . Hint Use lm to build a linear regression model and summary() to find the coefficients. From the lectures we know that the least squares estimates for the model parameters are \\[\\hat{\\boldsymbol \\beta} = \\begin{bmatrix} \\hat{\\alpha} \\\\[0.5em] \\hat{\\beta} \\end{bmatrix} = \\begin{bmatrix} \\bar{y} - \\hat{\\beta} \\bar{x} \\\\[0.5em] \\frac{S_{xy}}{S_{xx}} \\end{bmatrix}\\] Use this formula to compute the least squares estimates for the model parameters, \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\). \\(\\hat{\\alpha}\\) = , \\(\\hat{\\beta}\\) = Hint x&lt;-crime$Dropout y&lt;-crime$Crime x_mean &lt;- mean(x) y_mean &lt;- mean(y) S_xx &lt;- sum((x-x_mean)^2) S_xy &lt;- sum((x-x_mean)*(y-y_mean)) 2.1 Least squares estimates of model parameters using vector-matrix form To use the least squares vector-matrix formula given by (1.2), we first need to find the design matrix, \\(\\mathbf{X}\\). This can be done using the following R command: X &lt;- model.matrix(~Dropout, data = crime) This gives us the design matrix, \\(\\mathbf{X}\\), for the simple linear regression model in (1.1), where we have \\(n = 50\\) rows corresponding to each of the 50 US states, and \\(p = 2\\) columns corresponding to the model parameters \\(\\alpha\\) and \\(\\beta\\). More generally this is written as \\[\\mathbf{X} =\\begin{bmatrix} 1 &amp; x_1 \\\\1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n\\end{bmatrix} \\] The first column of \\(\\mathbf{X}\\) contains 1's as that is the column that multiplies the first component of the parameter vector \\(\\boldsymbol\\beta^\\top = \\begin{bmatrix} \\alpha &amp; \\beta \\end{bmatrix}\\), and as can be seen from the model, the intercept term, \\(\\alpha\\), is constant across all \\(i\\) observations. The slope parameter, \\(\\beta\\), is also constant, however, the differences in crime rates between states comes from changes in the percentage of high school dropouts, given by \\(x_i\\), which multiplies \\(\\beta\\). The random errors, \\(\\epsilon_i\\), also differ per state. Next we need the following commands to calculate each component in the least squares vector-matrix formula: t # gets the transpose of a vector or matrix %*% # multiplies matrices together solve # computes the inverse of a matrix Let's compute \\(\\mathbf{X}^\\top\\mathbf{X}\\) using the following R code: XtX &lt;- t(X) %*% X This gives \\[\\mathbf{X}^\\top\\mathbf{X} = \\begin{bmatrix} 50.0 &amp; 512.6 \\\\ 512.6 &amp; 5538.8 \\end{bmatrix}\\] Recall, if \\[\\mathbf{A} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix} \\quad\\quad \\text{then} \\quad\\quad \\mathbf{A}^{-1} = \\frac{1}{\\text{det}\\left(\\mathbf{A}\\right)}\\begin{bmatrix} d &amp; -b \\\\ -c &amp; a \\end{bmatrix} = \\frac{1}{ad - bc}\\begin{bmatrix} d &amp; -b \\\\ -c &amp; a \\end{bmatrix}.\\] Given \\(\\mathbf{X}^\\top\\mathbf{X}\\) above, compute its inverse \\(\\left(\\mathbf{X}^\\top\\mathbf{X}\\right)^{-1}\\): \\[(\\mathbf{X}^\\top\\mathbf{X})^{-1} = \\hspace{9em}\\] To find \\(\\hat{\\boldsymbol\\beta}\\) we also need to find \\(\\mathbf{X}^\\top\\mathbf{Y}\\). Using the above commands, or by hand, compute \\(\\mathbf{X}^\\top\\mathbf{Y}\\): \\[\\mathbf{X}^\\top\\mathbf{Y} = \\begin{bmatrix} &amp; &amp; &amp; &amp; \\\\\\\\ \\\\ \\end{bmatrix}\\] Multiplying \\(\\left(\\mathbf{X}^\\top\\mathbf{X}\\right)^{-1}\\) and \\(\\mathbf{X}^\\top\\mathbf{Y}\\) then gives us the least squares estimates of the model parameters. By hand, compute the parameter estimates, \\(\\hat{\\boldsymbol\\beta}\\), such that \\[\\boldsymbol{\\hat{\\beta}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{Y} = \\] QUESTION 1 Use the R commands given above to compute \\(\\left(\\mathbf{X}^\\top\\mathbf{X}\\right)^{-1}\\) and \\(\\mathbf{X}^\\top\\mathbf{Y}\\), and hence \\(\\hat{\\boldsymbol\\beta}\\), and compare the output with your handwritten results. Solution XtX_inv &lt;- solve(XtX) Y &lt;- crime$Crime XtY &lt;- t(X) %*% Y beta.hat &lt;- solve(XtX) %*% XtY We can also obtain the vector of random errors, \\(\\boldsymbol \\epsilon\\), by taking the difference between the observed values, \\(\\mathbf{Y}\\), and the fitted values, \\(\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol\\beta}\\), using the command: residuals &lt;- Y - X %*% param.ests The residual sum of squares for the model can be obtained from formula (1.3) using the following commands: YtY &lt;- t(Y) %*% Y RSS &lt;- YtY - t(XtY) %*% param.ests Now compute the estimate of the error variance, \\(\\hat{\\sigma}^2\\): \\[\\hat{\\sigma}^2 = \\frac{\\text{RSS}}{n-p} = \\hspace{10em}\\] The estimate of standard deviation, i.e. \\(\\hat{\\sigma}\\), has been calculated by R when fitting the model, which is the Residual standard error produced from summary(model1). This can also be seen using the following command: summary(model1)$sigma QUESTION 1 Use one or more predictors alongside Dropout to build a multiple linear regression model for explaining Crime. Hint Use the graph found using the pairs() command in (a) to select predictors that appear suitable for describing Crime. Recall that a multiple linear regression model can be constructed using model &lt;- lm(Crime ~ Dropout + Predictor1 + Predictor2 + ..., data = crime) Calculate the least squares estimates of parameters in the new multiple linear regression model using the vector-matrix formulation. Solution The same steps can be followed as in (d), but the design matrix X has to be updated accordingly. Say we want to add Police and Prison variables to our model. We would then use the following code. X &lt;- model.matrix(~ Dropout + Police + Prison, data = crime) Y &lt;- crime$Crime XtX &lt;- t(X) %*% X XtY &lt;- t(X) %*% Y beta.hat &lt;- solve(XtX) %*% XtY beta.hat ## [,1] ## (Intercept) 1513.317509 ## Dropout 148.343452 ## Police 4.782505 ## Prison 2.794514 "],["exercise-1-the-taste-of-cheese.html", "3 Exercise 1: The taste of cheese", " 3 Exercise 1: The taste of cheese The cheese dataset is collected during an experiment involving the chemical constituents of cheese and its taste. It contains the concentrations of acetic acid, hydrogen sulphide (H2S) and lactic acid, as well as a subjective taste score. It is of interest to investigate the effects of the different acids on the taste score. TASK 1 Produce scatterplots of Taste against Lactic Acid and Taste against H2S. Now plot Taste against log(H2S), and against log(Lactic Acid)). Choose the best two variables among (H2S, log(H2S), Lactic Acid, log(Lactic Acid)) to explain Taste and construct a multiple linear regression model using them. Estimate the parameters in your chosen model using the general formula in vector-matrix notation and check they are same as the R output. Estimate the error variance using the general formula in vector-matrix notation and check that it is same as the R output. "],["optional-reparameterising-the-model.html", "4 (optional) Reparameterising the model 4.1 Multiple linear regression", " 4 (optional) Reparameterising the model In Chapter 3 of the lecture notes, we will learn that reparameterising a linear model can make some of the calculations simpler. Today let's try to reparameterise the simple linear regression model (1.1). Suppose we go from \\[Y_i = \\alpha + \\beta x_i + \\epsilon_i\\] \\[\\Big\\downarrow\\] \\[Y_i = \\alpha + \\beta (x_i - \\bar{x}) + \\epsilon_i\\] That is, we subtract the average % dropout rate from each of the observed rates for each state, thus mean centering the % dropout rate. However, as we will see, this changes our interpretation of the model results. QUESTION 2 Write down the reparameterised model using vector-matrix notation: \\[\\underbrace{\\begin{bmatrix} &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ \\end{bmatrix}}_{\\mathbf{Y}} = \\underbrace{\\begin{bmatrix} &amp; &amp; &amp; &amp; &amp; &amp; \\\\ &amp; &amp; &amp; &amp; &amp; &amp; \\\\ &amp; &amp; &amp; &amp; &amp; &amp; \\\\ &amp; &amp; &amp; &amp; &amp; &amp; \\\\ &amp; &amp; &amp; &amp; &amp; &amp; \\\\ \\end{bmatrix}}_{\\mathbf{X}} \\underbrace{\\begin{bmatrix} &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ \\end{bmatrix}}_{\\boldsymbol\\beta} + \\underbrace{\\begin{bmatrix} &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ \\end{bmatrix}}_{\\boldsymbol\\epsilon} \\] Figures below display scatterplots of crime rate against % dropout, and crime rate against mean-centred % dropout. What does this tell us about the relationship between crime rate and the mean-centred % dropout in comparison with the % dropout? Figure 4.1: Top: crime rate vs. % dropout. Bottom: crime rate vs. mean-centred % dropout. Figure 4.2: Top: crime rate vs. % dropout. Bottom: crime rate vs. mean-centred % dropout. We can fit the reparameterised model using the following commands: drop.centred &lt;- crime$Dropout - mean(crime$Dropout) model2 &lt;- lm(Y ~ drop.centred) and obtain the model summary using: summary(model2) Comparing the model summary from model1 and model2, we can see that all of the statistics are the same, except for those for the intercept term, \\(\\alpha\\). That is because, if we look again at the least squares estimates of the parameters (without reparameterisation) \\[\\hat{\\boldsymbol\\beta} = \\begin{bmatrix} \\hat{\\alpha} \\\\[0.5em] \\hat{\\beta} \\end{bmatrix} = \\begin{bmatrix} \\bar{y} - \\hat{\\beta} \\bar{x} \\\\[0.5em] \\frac{S_{xy}}{S_{xx}} \\end{bmatrix},\\] \\(\\bar{x} = \\sum_{i=1}^n x_i/n\\) denoted the mean % dropout. However, with the reparameterisation, \\(\\bar{x}\\), in this case, say \\(\\bar{x}_c = \\sum_{i=1}^n \\left(x_i - \\bar{x}\\right)/n = 0\\), and hence the estimate of the intercept term, \\(\\hat{\\alpha} = \\bar{y}\\), i.e. the average US crime rate (per 100, 000 people). The intercept for the reparameterised model now takes on more meaning. It is the average crime rate when the predictor(s) are zero. For example, in model2 the intercept \\(\\hat{\\alpha} = 5085.2\\), that is, the average crime rate, when the mean-centred % dropout is 0, which is equivalent to taking the average % dropout (i.e. \\(\\bar{x} = 10.252\\)) in model1. This is shown in figures below which display the fitted regression lines from both models, as well as the crime rate given the average % of high school dropouts. Figure 4.3: Top: crime rate vs. % dropout with fitted line. Bottom: crime rate vs. mean-centred % dropout with fitted line. The dashed lines denote the crime rate given the average % of high school dropouts. For model1 our interpretation was that for every 1% increase in the % of high school dropouts, the expected crime rate (per 100, 000) increases by 281.8. For model2 our interpretation gives the crime rate related to some deviation from the mean % of high school dropouts. For example, if the % dropout rate is 12%, then from model1 we obtain \\[\\text{Crime} = 2196.5 + 281.8 \\cdot 12 = 5578\\] For model2 a 12% dropout rate relates to being \\(12 - \\bar{x} = 1.748%\\) higher than the average % dropout rate, and hence we get \\[\\text{Crime} = 5085.2 + 281.8 \\cdot 1.748 = 5578\\] 4.1 Multiple linear regression We will now use the crime data to fit a multiple linear regression model, such that \\[Y_i = \\alpha + \\beta (x_{1i} - \\bar{x}_{1.}) + \\gamma (x_{2i} - \\bar{x}_{2.}) + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2), \\quad i = 1,\\ldots,n,\\] where the predictors, \\(x_{1i}\\) and \\(x_{2i}\\), are mean centred. QUESTION 2 Write down the multiple linear regression model using vector-matrix notation: \\[\\underbrace{\\begin{bmatrix} &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ \\end{bmatrix}}_{\\mathbf{Y}} = \\underbrace{\\begin{bmatrix} &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\\\ \\end{bmatrix}}_{\\mathbf{X}} \\underbrace{\\begin{bmatrix} &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ \\end{bmatrix}}_{\\boldsymbol\\beta} + \\underbrace{\\begin{bmatrix} &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ \\end{bmatrix}}_{\\boldsymbol\\epsilon} \\] Solution \\[\\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} = \\begin{bmatrix} 1 &amp; x_{11}-\\bar{x}_{1.} &amp; x_{21}-\\bar{x}_{2.} \\\\ 1 &amp; x_{12}-\\bar{x}_{1.} &amp; x_{22}-\\bar{x}_{2.} \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_{1n}-\\bar{x}_{1.} &amp; x_{2n}-\\bar{x}_{2.} \\\\ \\end{bmatrix} \\begin{bmatrix} \\alpha \\\\ \\beta \\\\ \\gamma \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\\] Reparameterise the multiple linear regression built in Question 1(e) and compare the least square estimates of parameters. Solution Recap that our original model is given by model1 &lt;- lm(Crime ~ Dropout + Police + Prison, data = crime) Now we fit the reparameterised model by using: police.centred &lt;- crime$Police - mean(crime$Police) prison.centred &lt;- crime$Prison - mean(crime$Prison) model2 &lt;- lm(Crime ~ drop.centred + police.centred + prison.centred, data = crime) Apart from the intercept, the parameters remain the same. coef(model1) ## (Intercept) Dropout Police Prison ## 1513.317509 148.343452 4.782505 2.794514 coef(model2) ## (Intercept) drop.centred police.centred prison.centred ## 5085.160000 148.343452 4.782505 2.794514 "],["solution.html", "5 Solution 5.1 Task 1", " 5 Solution 5.1 Task 1 # taste vs lactic acid plot(Taste ~ Lactic.Acid, data = cheese, xlab = &quot;Lactic acid concentration&quot;, ylab = &quot;Taste score&quot;) # taste vs H2S plot(Taste ~ H2S, data = cheese, xlab = &quot;H2S concentration&quot;, ylab = &quot;Taste score&quot;) # taste vs log(lactic acid) plot(Taste ~ log(Lactic.Acid), data = cheese, xlab = &quot;Log lactic acid concentration&quot;, ylab = &quot;Taste score&quot;) # taste vs log(H2S) plot(Taste ~ log(H2S), data = cheese, xlab = &quot;Log H2S concentration&quot;, ylab = &quot;Taste score&quot;) Based on the above four plots, it is clear that log(H2S) is preferred over H2S in explaining Taste. The difference between Lactic Acid and log(Lactic Acid)) is little and therefore we will select the original variable Lactic Acid as the predictor. To build a multiple linear regression between Taste (as the response) and Lactic Acid and log(H2S) (as the predictors), we use model &lt;- lm(Taste ~ Lactic.Acid + log(H2S), data=cheese) summary(model) ## ## Call: ## lm(formula = Taste ~ Lactic.Acid + log(H2S), data = cheese) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.343 -6.530 -1.164 4.844 25.618 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -27.592 8.982 -3.072 0.00481 ** ## Lactic.Acid 19.887 7.959 2.499 0.01885 * ## log(H2S) 3.946 1.136 3.475 0.00174 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.942 on 27 degrees of freedom ## Multiple R-squared: 0.6517, Adjusted R-squared: 0.6259 ## F-statistic: 25.26 on 2 and 27 DF, p-value: 6.551e-07 ## [,1] ## (Intercept) -27.591815 ## Lactic.Acid 19.887204 ## log(H2S) 3.946267 ## (Intercept) Lactic.Acid log(H2S) ## -27.591815 19.887204 3.946267 # calculate according to the formula YtY &lt;- t(Y) %*% Y RSS &lt;- YtY - t(XtY) %*% beta.hat sigma2_hat &lt;- RSS/(nrow(cheese)-3) sigma2_hat ## [,1] ## [1,] 98.85057 # from R output summary(model)$sigma ## [1] 9.942362 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
